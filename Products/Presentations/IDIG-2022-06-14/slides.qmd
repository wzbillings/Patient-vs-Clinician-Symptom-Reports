---
format: revealjs
---

## 

<br>

<br>

:::{.r-fit-text}
**Applicability of diagnostic risk models**

**for influenza triage and telemedicine**
:::

Zane Billings

Coauthors: Annika Cleven, Ariella Perry Dale, Jacqueline Dworaczyk, Mark Ebell, Andreas Handel, Brian McKay

## Intro

::: {.incremental}
* Telemedicine, triage, and flu diagnosis
* Patients and clinicians often disagree on symptom reports
* Clinical prediction rules are typically built using clinician-reported data
* **Our question: how much do patients and clinicians disagree? Enough to make CPRs ineffective? If so, can we develop better models?**
:::

## Data source

* Data were collected from UGA UHC from December 2016 -- February 2017.
* Patients who reported a respiratory complaint were required to complete a survey. Included reporting a set of symptoms as present or absent.
* Captured EHR data where clinicians were required to check the same set of symptoms.
* Final clinician diagnosis was recorded, and some patients got either a rapid test or PCR test also.

# Part 1: Interrater agreement and previous model performance

## Symptom report agreement

* We assessed the level of agreement between patient and clinician reports using **Cohen's $\kappa$** statistic, which accounts for agreement due to chance alone.
* $\kappa \in [-1,1]$ where $\kappa = 1$ is perfect agreement, $\kappa = 0$ is "random" agreement, and $\kappa = -1$ is perfect disagreement.
* Some research suggests that $\kappa \geq 0.8$ is an appropriate threshold for "agreement" in a clinical setting^[McHugh, Mary L. “Interrater Reliability: The Kappa Statistic.” Biochemia Medica 22, no. 3 (October 15, 2012): 276–82.].

## Symptom report agreement

![](Figures/kappas.png) 

## Symptom report agreement

![](Figures/kappas_sub.png) 

## Previously reported CPRs {.smaller}

We choose five clinical prediction rules from the existing literature.

1. **CF**: Cough and fever
2. **CFA**: Cough, fever, and acute onset of disease
3. **CFM**: Cough, fever, and myalgias^[1-3 from Monto AS, Gravenstein S, Elliott M, et al. Arch Intern Med. 2000;160(21):3243-3247. DOI: 10.1001/archinte.160.21.3243]
4. **Score**: Weighted score from regression^[Ebell MH, Afonso AM, Gonzales R, et al. JABFM. 2012;25(1):55-62. DOI: 10.3122/jabfm.2012.01.110161]
5. **Tree**: Fast and frugal tree^[Afonso AM, Ebell MH, Gonzales R, et al. Family Practice. 2012;29(6):671-677. DOI: 10.1093/fampra/cms020]

We used all five models to get predicted diagnosis from patient-reported symptoms, and from clinician-reported symptoms.

## Previously reported CPR performance

We compared the previously-published AUROCC to the AUROCC of the predictions for our data.

```{r}
knitr::kable(readRDS(here::here("Results/Tables/AUC-table.Rds")))
```

## Previously reported CPR performance

![](Figures/auc_cis_plot.png)

## Previously reported CPR performance

:::{.incremental}
* Interestingly, our patient AUCs are similar to previously published, but clinician AUCs are higher. **Why?**
1. The AUCs don't tell the whole story -- we have to choose cutoffs and compare a different way.
2. Our main outcome is **clinician diagnosis.** Clinicians will be biased -- most scores also use PCR, so what if we compare to lab results?
:::

# Part 2: Looking beyond AUCs

## Thresholds and metrics

:::{.incremental}
* While AUC is easy and nice for researchers, clinicians can't really use it. We need to choose thresholds for our models.
* Prior research^[Sintchenko V, et al. Journal of Clinical Virology. 2002;25(1):15-21. doi:10.1016/S1386-6532(00)00182-7 and Ebell MH, et al. JABFM. 2012;25(1):55-62. DOI: 10.3122/jabfm.2012.01.110161] suggest a no test/test threshold of 10% and a test/treat threshold of 50%.
* So, we used a risk of 50% or higher as our diagnostic threshold. We plan to investigate the 10% threshold in the future.
:::

## Thresholds and metrics {.scrollable}
* Once we have binary diagnoses, there are several metrics we could use to quantify how "good" a test is.
* AUC with one cutpoint = **balanced accuracy**
$$\mathrm{BA} = \frac{\text{sens} + \text{spec}}{2}$$
* Some research suggests **Matthews Correlation Coefficient** (MCC, $\phi$) is most robust, but it is prevalence dependent.
$$\mathrm{MCC} = \text{Corr}\left(\mathrm{truth}, \mathrm{estimate}\right)$$
* **$J$-index** ("informedness") may be best when comparing across datasets with variable prevalences.
$$J = \text{sens} + \text{spec} - 1$$
* Since influenza prevalence varies widely, we'll use the **$J$-index** for our comparisons, but we calculated a suite of performance metrics.
* Assessing calibration is also extremely important, and we plan to do this in the future.

:::{.aside}
See Chicco D, Jurman G. BMC Genomics. 2020;21:6. doi:10.1186/s12864-019-6413-7 and Chicco D, Tötsch N, Jurman G. BioData Mining. 2021;14(1):13. doi:10.1186/s13040-021-00244-z
:::

## Observed vs. previous $J$-indices

![](Figures/J-plot.png) 

# Part 3: Sample size vs gold standard

## Table 1: Subsets recieving lab tests

```{r}
readRDS(here::here("Results/Tables/table_one.Rds"))
```

* Clinicians appear to have negative bias for flu diagnosis in our sample.
* PCR patients were a convenience subsample following the same inclusion/exclusion criteria.^[ See Dale AP, et al. JABFM 2019;32(2):226-233. doi:10.3122/jabfm.2019.02.180183 for details. No details provided on RADT administration.]

## Clinician/lab test agreement

```{r}
readRDS(here::here("Results/Tables/DxMethodKappas.Rds")) |>
  knitr::kable()
```

* Clinicians and RADT/RIDT disagree often. These tests often have low sensitivity^[Trombetta VK, et al. Hawaii J Med Public Health. 2018;77(9):226-230]. We observed moderate agreement between clinicians and PCR, the "gold standard."
* Clinicians observed lab results prior to diagnosis.

## AUC when lab test is the outcome

![](Figures/lab_auc_cis_plot.png) 

## $J$-index when lab test is the outcome

![](Figures/j-plot-lab.png)

## Conclusions and future work {.scrollable}

* When clinician diagnosis is the outcome, clinician symptom ratings have more predictive power than patient symptom ratings (as expected).
* There are interesting results with the previously-published performance metrics that we need to look into further. Potentially using the 10% threshold instead of 50% could explain this, but we haven't checked yet.
* We also need to assess model calibration, which is arguably more important than performance metrics. We could scale predictions to recalibrate if necessary.
* We plan to train risk models using patient data to see if we can get better performance than the previously reported models (Annika will be doing most of this).
* **Overall conclusion**: Previous diagnostic risk models appear to have inconsistent performance. It is difficult to determine how useful these models will be for telemedicine without more exploration.
* Home testing may be available in the future, but symptom-based models would be much cheaper and easier to implement. However, different test/treat thresholds may be necessary^[Cai X, et al. BMC Prim Care. 2022;23:75. doi:10.1186/s12875-022-01675-1].
* we need to figure out what metrics are appropriate for us to use.
* Figure 1, add N for yes/no for both groups
* Mark symptoms in fig 1 that are used for scores, skip fig 2
* Model 1 of Afonso 2012 is not a FFT
* be specific about details from previous models
* deep dive on metrics
